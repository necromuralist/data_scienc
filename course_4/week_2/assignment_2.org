#+TITLE: Assignment 2 - Introduction to NLTK

In part 1 of this assignment you will use nltk to explore the Herman Melville novel Moby Dick. Then in part 2 you will create a spelling recommender function that uses nltk to find words similar to the misspelling. 

* Part 1 - Analyzing Moby Dick

** Imports

#+BEGIN_SRC ipython :session assignment2 :results none
from nltk.stem import WordNetLemmatizer
from nltk.probability import FreqDist
import nltk
import pandas
import numpy
#+END_SRC

If you would like to work with the raw text you can use 'moby_raw'.

#+BEGIN_SRC ipython :session assignment2 :results none
with open('moby.txt', 'r') as reader:
    moby_raw = reader.read()
#+END_SRC
    
If you would like to work with the novel in =nltk.Text= format you can use 'text1'.

#+BEGIN_SRC ipython :session assignment2 :results none
moby_tokens = nltk.word_tokenize(moby_raw)
text1 = nltk.Text(moby_tokens)
moby_series = pandas.Series(moby_tokens)
#+END_SRC

** Examples
*** Example 1
   /How many tokens (words and punctuation symbols) are in text1?/ A *token* is a linguistic unit such as a word, punctuation mark, or alpha-numeric strings.

   *This function should return an integer.*

#+BEGIN_SRC ipython :session assignment2 :results output
def example_one():
     """counts the tokens in moby dick

     Returns:
      int: number of tokens in moby dick
     """
     # or alternatively len(text1)
     return len(nltk.word_tokenize(moby_raw))

MOBY_TOKEN_COUNT = example_one()
print("Moby Dick has {:,} tokens.".format(
     MOBY_TOKEN_COUNT))

#+END_SRC

#+RESULTS:
: Moby Dick has 254,989 tokens.

*** Example 2

/How many unique tokens (unique words and punctuation) does text1 have?/

*This function should return an integer.*

#+BEGIN_SRC ipython :session assignment2 :results output
def example_two():
    """counts the unique tokens

    Returns:
     int: count of unique tokens in Moby Dick
    """
    # or alternatively len(set(text1))
    return len(set(nltk.word_tokenize(moby_raw)))

MOBY_UNIQUE_COUNT = example_two()
print("Moby Dick has {:,} unique tokens.".format(
    MOBY_UNIQUE_COUNT))
#+END_SRC

#+RESULTS:
: Moby Dick has 20,755 unique tokens.

*** Example 3

/After lemmatizing the verbs, how many unique tokens does text1 have?/ A *lemma* is the canonical form. e.g. /run/ is the lemma for /runs/, /ran/, /running/, and /run/.

*This function should return an integer.*

#+BEGIN_SRC ipython :session assignment2 :results output
def example_three():
    """Counts the number of lemma in Moby Dick

    Returns:
     int: count of unique lemma
    """
    lemmatizer = WordNetLemmatizer()
    return len(set([lemmatizer.lemmatize(w,'v') for w in text1]))

MOBY_LEMMA_COUNT = example_three()
print("Moby Dick has {:,} lemma (found in WordNet).".format(
    MOBY_LEMMA_COUNT))

#+END_SRC

#+RESULTS:
: Moby Dick has 16,900 lemma (found in WordNet).

** Questions
*** Question 1

What is the lexical diversity of the given text input? (i.e. ratio of unique tokens to the total number of tokens)
 
*This function should return a float.*

#+BEGIN_SRC ipython :session assignment2 :results output
def answer_one():
    """Calculates the lexical diversity of Moby Dick
    
    Returns:
     float: fraction of tokens that are unique
    """    
    return MOBY_UNIQUE_COUNT/float(MOBY_TOKEN_COUNT)

lexical_diversity = answer_one()
print("Lexical Diversity of Moby Dick: {:.2f}".format(lexical_diversity))
#+END_SRC

#+RESULTS:
: Lexical Diversity of Moby Dick: 0.08

About 8 percent of the tokens in Moby Dick are unique.

*** Question 2

/What percentage of tokens is 'whale'or 'Whale'?/

*This function should return a float.*

#+BEGIN_SRC ipython :session assignment2 :results output
def answer_two():
    """calculates percentage of tokens that are 'whale'

    Returns:
     float: fraction of 'whale'
    """
    whales = moby_series.str.extractall("([wW]hale)")
    return len(whales)/float(MOBY_TOKEN_COUNT)

whale_fraction = answer_two()
print("Fraction of tokens that are whales: {:.2f}".format(whale_fraction))
#+END_SRC

#+RESULTS:
: Fraction of tokens that are whales: 0.01

Around 1 percent of the tokens are 'whale'.

*** Question 3

/What are the 20 most frequently occurring (unique) tokens in the text? What is their frequency?/

*This function should return a list of 20 tuples where each tuple is of the form `(token, frequency)`. The list should be sorted in descending order of frequency.*

#+BEGIN_SRC ipython :session assignment2 :results none
moby_frequencies = FreqDist(moby_tokens)
#+END_SRC

#+BEGIN_SRC ipython :session assignment2 :results output
def answer_three():
    """finds 20 most requently occuring tokens

    Returns:
     list: (token, frequency) for top 20 tokens
    """
    return frequencies.most_common(20)

print(answer_three())
#+END_SRC

#+RESULTS:
: [(',', 19204), ('the', 13715), ('.', 7308), ('of', 6513), ('and', 6010), ('a', 4545), ('to', 4515), (';', 4173), ('in', 3908), ('that', 2978), ('his', 2459), ('it', 2196), ('I', 2097), ('!', 1767), ('is', 1722), ('--', 1713), ('with', 1659), ('he', 1658), ('was', 1639), ('as', 1620)]

*** Question 4

/What tokens have a length of greater than 5 and frequency of more than 150?/

*This function should return a sorted list of the tokens that match the above constraints. To sort your list, use `sorted()`*

#+BEGIN_SRC ipython :session assignment2 :results none
moby_frequency_frame = pandas.DataFrame(moby_frequencies.most_common(),
                                        columns=["token", "frequency"])
#+END_SRC

#+BEGIN_SRC ipython :session assignment2 :results none
def answer_four():
    """gets tokens with length > 5, frequency > 150"""
    return moby_frequency_frame[(moby_frequency_frame.frequency > 150)
                                  & (moby_frequency_frame.token.str.len() > 5)]

output = answer_four()
assert all(output > 15)
assert all(output.token.str.len() > 5)
#+END_SRC

*** Question 5

    /Find the longest word in text1 and that word's length./
 
*This function should return a tuple `(longest_word, length)`.*

#+BEGIN_SRC ipython :session assignment2 :results output
def answer_five():
    """finds the longest word and its length

    Return:
     tuple: (longest-word, length)
    """
    length = max(moby_frequency_frame.token.str.len())
    longest = moby_frequency_frame.token.str.extractall("(?P<long>.{{{}}})".format(length))
    return (longest.long.iloc[0], length)

print(answer_five())
#+END_SRC

#+RESULTS:
: ("twelve-o'clock-at-night", 23)

*** Question 6

What unique words have a frequency of more than 2000? What is their frequency?

Hint:  you may want to use `isalpha()` to check if the token is a word and not punctuation.

*This function should return a list of tuples of the form `(frequency, word)` sorted in descending order of frequency.*

#+BEGIN_SRC ipython :session assignment2 :results output
def answer_six():
    """Finds words wih frequency > 2000

    Returns:
     list: frequency, word tuples
    """
    words = moby_frequency_frame[moby_frequency_frame.token.str.isalpha()]
    common = words[words.frequency > 2000]
    return [tuple(row) for row in common.values]

print(answer_six())
#+END_SRC

#+RESULTS:
: [('the', 13715), ('of', 6513), ('and', 6010), ('a', 4545), ('to', 4515), ('in', 3908), ('that', 2978), ('his', 2459), ('it', 2196), ('I', 2097)]

*** Question 7

/What is the average number of tokens per sentence?/
 
*This function should return a float.*

#+BEGIN_SRC ipython :session assignment2 :results output
def answer_seven():
    """average number of tokens per sentence"""
    
    return # Your answer here

answer_seven()
#+END_SRC

# ### Question 8
# 
# What are the 5 most frequent parts of speech in this text? What is their frequency?
# 
# *This function should return a list of tuples of the form `(part_of_speech, frequency)` sorted in descending order of frequency.*

# In[ ]:


def answer_eight():
    
    
    return # Your answer here

answer_eight()


# ## Part 2 - Spelling Recommender
# 
# For this part of the assignment you will create three different spelling recommenders, that each take a list of misspelled words and recommends a correctly spelled word for every word in the list.
# 
# For every misspelled word, the recommender should find find the word in `correct_spellings` that has the shortest distance*, and starts with the same letter as the misspelled word, and return that word as a recommendation.
# 
# *Each of the three different recommenders will use a different distance measure (outlined below).
# 
# Each of the recommenders should provide recommendations for the three default words provided: `['cormulent', 'incendenece', 'validrate']`.

# In[ ]:


from nltk.corpus import words

correct_spellings = words.words()


# ### Question 9
# 
# For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:
# 
# **[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the trigrams of the two words.**
# 
# *This function should return a list of length three:
# `['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*

# In[ ]:


def answer_nine(entries=['cormulent', 'incendenece', 'validrate']):
    
    
    return # Your answer here
    
answer_nine()


# ### Question 10
# 
# For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:
# 
# **[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the 4-grams of the two words.**
# 
# *This function should return a list of length three:
# `['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*

# In[ ]:


def answer_ten(entries=['cormulent', 'incendenece', 'validrate']):
    
    
    return # Your answer here
    
answer_ten()


# ### Question 11
# 
# For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:
# 
# **[Edit distance on the two words with transpositions.](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)**
# 
# *This function should return a list of length three:
# `['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*

# In[ ]:


def answer_eleven(entries=['cormulent', 'incendenece', 'validrate']):
    
    
    return # Your answer here 
    
answer_eleven()

[fn:1] Nitin Madnani. 2007. Getting started on natural language processing with Python. Crossroads 13, 4 (September 2007), 5-5. DOI=http://dx.doi.org/10.1145/1315325.1315330
