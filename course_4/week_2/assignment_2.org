#+TITLE: Assignment 2 - Introduction to NLTK

In part 1 of this assignment you will use nltk to explore the Herman Melville novel Moby Dick. Then in part 2 you will create a spelling recommender function that uses nltk to find words similar to the misspelling. 

* Part 1 - Analyzing Moby Dick
** Imports

#+BEGIN_SRC ipython :session assignment2 :results none
from nltk.stem import WordNetLemmatizer
from nltk.probability import FreqDist
from nltk.tokenize import sent_tokenize
import nltk
import nltk.data
import pandas
import numpy
#+END_SRC

If you would like to work with the raw text you can use 'moby_raw'.

#+BEGIN_SRC ipython :session assignment2 :results none
with open('moby.txt', 'r') as reader:
    moby_raw = reader.read()
#+END_SRC
    
If you would like to work with the novel in =nltk.Text= format you can use 'text1'.

#+BEGIN_SRC ipython :session assignment2 :results none
moby_tokens = nltk.word_tokenize(moby_raw)
text1 = nltk.Text(moby_tokens)
moby_series = pandas.Series(moby_tokens)
#+END_SRC

** Examples
*** Example 1
   /How many tokens (words and punctuation symbols) are in text1?/ A *token* is a linguistic unit such as a word, punctuation mark, or alpha-numeric strings.

   *This function should return an integer.*

#+BEGIN_SRC ipython :session assignment2 :results output
def example_one():
     """counts the tokens in moby dick

     Returns:
      int: number of tokens in moby dick
     """
     # or alternatively len(text1)
     return len(nltk.word_tokenize(moby_raw))

MOBY_TOKEN_COUNT = example_one()
print("Moby Dick has {:,} tokens.".format(
     MOBY_TOKEN_COUNT))

#+END_SRC

#+RESULTS:
: Moby Dick has 254,989 tokens.

*** Example 2

/How many unique tokens (unique words and punctuation) does text1 have?/

*This function should return an integer.*

#+BEGIN_SRC ipython :session assignment2 :results output
def example_two():
    """counts the unique tokens

    Returns:
     int: count of unique tokens in Moby Dick
    """
    # or alternatively len(set(text1))
    return len(set(nltk.word_tokenize(moby_raw)))

MOBY_UNIQUE_COUNT = example_two()
print("Moby Dick has {:,} unique tokens.".format(
    MOBY_UNIQUE_COUNT))
#+END_SRC

#+RESULTS:
: Moby Dick has 20,755 unique tokens.

*** Example 3

/After lemmatizing the verbs, how many unique tokens does text1 have?/ A *lemma* is the canonical form. e.g. /run/ is the lemma for /runs/, /ran/, /running/, and /run/.

*This function should return an integer.*

#+BEGIN_SRC ipython :session assignment2 :results output
def example_three():
    """Counts the number of lemma in Moby Dick

    Returns:
     int: count of unique lemma
    """
    lemmatizer = WordNetLemmatizer()
    return len(set([lemmatizer.lemmatize(w,'v') for w in text1]))

MOBY_LEMMA_COUNT = example_three()
print("Moby Dick has {:,} lemma (found in WordNet).".format(
    MOBY_LEMMA_COUNT))

#+END_SRC

#+RESULTS:
: Moby Dick has 16,900 lemma (found in WordNet).

** Questions
*** Question 1

What is the lexical diversity of the given text input? (i.e. ratio of unique tokens to the total number of tokens)
 
*This function should return a float.*

#+BEGIN_SRC ipython :session assignment2 :results output
def answer_one():
    """Calculates the lexical diversity of Moby Dick
    
    Returns:
     float: fraction of tokens that are unique
    """    
    return MOBY_UNIQUE_COUNT/float(MOBY_TOKEN_COUNT)

lexical_diversity = answer_one()
print("Lexical Diversity of Moby Dick: {:.2f}".format(lexical_diversity))
#+END_SRC

#+RESULTS:
: Lexical Diversity of Moby Dick: 0.08

About 8 percent of the tokens in Moby Dick are unique.

*** Question 2

/What percentage of tokens is 'whale'or 'Whale'?/

*This function should return a float.*

#+BEGIN_SRC ipython :session assignment2 :results output
def answer_two():
    """calculates percentage of tokens that are 'whale'

    Returns:
     float: fraction of 'whale'
    """
    whales = moby_series.str.extractall("([wW]hale)")
    return len(whales)/float(MOBY_TOKEN_COUNT)

whale_fraction = answer_two()
print("Fraction of tokens that are whales: {:.2f}".format(whale_fraction))
#+END_SRC

#+RESULTS:
: Fraction of tokens that are whales: 0.01

Around 1 percent of the tokens are 'whale'.

*** Question 3

/What are the 20 most frequently occurring (unique) tokens in the text? What is their frequency?/

*This function should return a list of 20 tuples where each tuple is of the form `(token, frequency)`. The list should be sorted in descending order of frequency.*

#+BEGIN_SRC ipython :session assignment2 :results none
moby_frequencies = FreqDist(moby_tokens)
#+END_SRC

#+BEGIN_SRC ipython :session assignment2 :results output
def answer_three():
    """finds 20 most requently occuring tokens

    Returns:
     list: (token, frequency) for top 20 tokens
    """
    return moby_frequencies.most_common(20)

print(answer_three())
#+END_SRC

#+RESULTS:
: [(',', 19204), ('the', 13715), ('.', 7308), ('of', 6513), ('and', 6010), ('a', 4545), ('to', 4515), (';', 4173), ('in', 3908), ('that', 2978), ('his', 2459), ('it', 2196), ('I', 2097), ('!', 1767), ('is', 1722), ('--', 1713), ('with', 1659), ('he', 1658), ('was', 1639), ('as', 1620)]

*** Question 4

/What tokens have a length of greater than 5 and frequency of more than 150?/

*This function should return a sorted list of the tokens that match the above constraints. To sort your list, use `sorted()`*

#+BEGIN_SRC ipython :session assignment2 :results none
moby_frequency_frame = pandas.DataFrame(moby_frequencies.most_common(),
                                        columns=["token", "frequency"])
#+END_SRC

#+BEGIN_SRC ipython :session assignment2 :results none
def answer_four():
    """gets tokens with length > 5, frequency > 150"""
    return moby_frequency_frame[(moby_frequency_frame.frequency > 150)
                                  & (moby_frequency_frame.token.str.len() > 5)]

output = answer_four()
assert all(output > 15)
assert all(output.token.str.len() > 5)
#+END_SRC

*** Question 5

    /Find the longest word in text1 and that word's length./
 
*This function should return a tuple `(longest_word, length)`.*

#+BEGIN_SRC ipython :session assignment2 :results output
def answer_five():
    """finds the longest word and its length

    Return:
     tuple: (longest-word, length)
    """
    length = max(moby_frequency_frame.token.str.len())
    longest = moby_frequency_frame.token.str.extractall("(?P<long>.{{{}}})".format(length))
    return (longest.long.iloc[0], length)

print(answer_five())
#+END_SRC

#+RESULTS:
: ("twelve-o'clock-at-night", 23)

*** Question 6

What unique words have a frequency of more than 2000? What is their frequency?

Hint:  you may want to use `isalpha()` to check if the token is a word and not punctuation.

*This function should return a list of tuples of the form `(frequency, word)` sorted in descending order of frequency.*

#+BEGIN_SRC ipython :session assignment2 :results none
moby_words = moby_frequency_frame[moby_frequency_frame.token.str.isalpha()]
moby_words.token = moby_words.token.str.lower()
#+END_SRC

#+BEGIN_SRC ipython :session assignment2 :results output
def answer_six():
    """Finds words wih frequency > 2000

    Returns:
     list: frequency, word tuples
    """
    common = moby_words[moby_words.frequency > 2000]
    return [tuple(row) for row in common.values]

print(answer_six())
#+END_SRC

#+RESULTS:
: [('the', 13715), ('of', 6513), ('and', 6010), ('a', 4545), ('to', 4515), ('in', 3908), ('that', 2978), ('his', 2459), ('it', 2196), ('i', 2097)]

*** Question 7

/What is the average number of tokens per sentence?/
 
*This function should return a float.*

#+BEGIN_SRC ipython :session assignment2 :results output
def answer_seven():
    """average number of tokens per sentence"""
    sentences = sent_tokenize(moby_raw)
    counts = (len(nltk.word_tokenize(sentence)) for sentence in sentences)
    return sum(counts)/float(len(sentences))

output = answer_seven()
print("Average number of tokens per sentence: {:.2f}".format(output))
#+END_SRC

#+RESULTS:
: Average number of tokens per sentence: 25.88

*** Question 8

/What are the 5 most frequent parts of speech in this text? What is their frequency?/ Parts of Speech (POS) are the lexical categories that words belong to.

*This function should return a list of tuples of the form `(part_of_speech, frequency)` sorted in descending order of frequency.*

#+BEGIN_SRC ipython :session assignment2 :results output
def answer_eight():
    """gets the 5 most frequent parts of speech

    Returns:
     list (Tuple): (part of speech, frequency) for top 5
    """
    tags = nltk.pos_tag(moby_words.token)
    frequencies = FreqDist([tag for (word, tag) in tags])
    return frequencies.most_common(5)

output = answer_eight()
print("Top 5 parts of speech: {}".format(output))
#+END_SRC

#+RESULTS:
: Top 5 parts of speech: [('NN', 4969), ('JJ', 3731), ('NNS', 2846), ('VBG', 1519), ('RB', 1295)]

* Part 2 - Spelling Recommender

For this part of the assignment you will create three different spelling recommenders, that each take a list of misspelled words and recommends a correctly spelled word for every word in the list.

For every misspelled word, the recommender should find find the word in `correct_spellings` that has the shortest distance*, and starts with the same letter as the misspelled word, and return that word as a recommendation.

*Each of the three different recommenders will use a different distance measure (outlined below)*.

Each of the recommenders should provide recommendations for the three default words provided: `['cormulent', 'incendenece', 'validrate']`.

#+BEGIN_SRC ipython :session assignment2 :results none
from nltk.corpus import words
from nltk.metrics.distance import (
    edit_distance,
    jaccard_distance,
    )
from nltk.util import ngrams
#+END_SRC

#+BEGIN_SRC ipython :session assignment2 :results none
correct_spellings = words.words()
#+END_SRC

** Question 9
For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:

**[[https://en.wikipedia.org/wiki/Jaccard_index][Jaccard distance]] on the trigrams of the two words.**

*This function should return a list of length three:
 `['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*

#+BEGIN_SRC ipython :session assignment2 :results output
def answer_nine(entries=['cormulent', 'incendenece', 'validrate']):
    """finds the closest word based on jaccard distance"""
    outcomes = []
    for entry in entries:
        distances = ((jaccard_distance(set(nltk.trigrams(entry)),
                                       set(nltk.trigrams(word))), word)
                     for word in correct_spellings)
        closest = min(distances)
        outcomes.append(closest[1])
    return outcomes
    
print(answer_nine())
#+END_SRC

#+RESULTS:
: ['formule', 'ascendence', 'validate']

** Question 10

For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:

**[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the 4-grams of the two words.**

*This function should return a list of length three:
 `['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*

#+BEGIN_SRC ipython :session assignment2 :results output
def answer_ten(entries=['cormulent', 'incendenece', 'validrate']):
    """gets the neares words using jaccard-distance with 4-grams

    Args:
     entries (list): words to find nearest other word for
    
    Returns:
     list: nearest words found
    """
    outcomes = []
    for entry in entries:
        distances = ((jaccard_distance(set(ngrams(entry, 4)),
                                       set(ngrams(word, 4))), word)
                     for word in correct_spellings)
        closest = min(distances)
        outcomes.append(closest[1])
    return outcomes
    
print(answer_ten())
#+END_SRC

#+RESULTS:
: ['formule', 'ascendent', 'drate']

** Question 11

For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:

**[[https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance][Edit (Levenshtein) distance on the two words with transpositions.]]**

*This function should return a list of length three:
 `['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*

#+BEGIN_SRC ipython :session assignment2 :results output
def answer_eleven(entries=['cormulent', 'incendenece', 'validrate']):
    """gets the nearest words based on Levenshtein distance

    Args:
     entries (list[str]): words to find closest words to

    Returns:
     list[str]: nearest words to the entries
    """
    outcomes = []
    for entry in entries:
        distances = ((edit_distance(entry,
                                    word), word)
                     for word in correct_spellings)
        closest = min(distances)
        outcomes.append(closest[1])
    return outcomes
    
print(answer_eleven())
#+END_SRC

#+RESULTS:
: ['corpulent', 'intendence', 'validate']

* Sources
[fn:1] Nitin Madnani. 2007. Getting started on natural language processing with Python. Crossroads 13, 4 (September 2007), 5-5. DOI=http://dx.doi.org/10.1145/1315325.1315330
