#+TITLE: Selecting the Email Management Classification Model

* Imports

#+BEGIN_SRC ipython :session emailmodel :results none
# pypi
from sklearn.ensemble import (
    ExtraTreesClassifier,
    RandomForestClassifier,
    )
from sklearn.feature_selection import (
    RFECV,
    SelectFromModel,
)
from sklearn.linear_model import LogisticRegressionCV
from sklearn.model_selection import (
    GridSearchCV,
    StratifiedKFold,
    train_test_split,
    )

from sklearn.dummy import DummyClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
import matplotlib.pyplot as pyplot
import mglearn
import numpy
import pandas
import seaborn
#+END_SRC

#+BEGIN_SRC ipython :session emailmodel :results none
% matplotlib inline
seaborn.set_style("whitegrid")
#+END_SRC

* The Data

#+BEGIN_SRC ipython :session emailmodel :results none
data = pandas.read_hdf("email_data.h5", "df")
cleaned_data = data[pandas.notnull(data.management)]
#+END_SRC

#+BEGIN_SRC ipython :session emailmodel :results output
print(cleaned_data.head())
#+END_SRC

#+RESULTS:
#+begin_example
   department  management  clustering  degree  degree_centrality  \
0           1         0.0    0.276423      44           0.043825   
3          21         1.0    0.384910      71           0.070717   
4          21         1.0    0.318691      96           0.095618   
6          25         1.0    0.155183     115           0.114542   
7          14         0.0    0.287785      72           0.071713   

   closeness_centrality  betweenness_centrality  pagerank  authority  
0              0.421991                0.001124  0.001224   0.000944  
3              0.441663                0.001654  0.001833   0.002369  
4              0.462152                0.005547  0.002526   0.003055  
6              0.475805                0.012387  0.003146   0.002554  
7              0.420156                0.002818  0.002002   0.001155  
#+end_example

** Department
   Even though I don't think it's going to prove useful, the =department= feature is actually categorical, despite the use of integers so we'll have to add dummy variables for it.

#+BEGIN_SRC ipython :session emailmodel :results output
cleaned_data = pandas.get_dummies(cleaned_data, columns=["department"])
print(cleaned_data.head(1))
#+END_SRC

#+RESULTS:
#+begin_example
   management  clustering  degree  degree_centrality  closeness_centrality  \
0         0.0    0.276423      44           0.043825              0.421991   

   betweenness_centrality  pagerank  authority  department_0  department_1  \
0                0.001124  0.001224   0.000944             0             1   

       ...        department_32  department_33  department_34  department_35  \
0      ...                    0              0              0              0   

   department_36  department_37  department_38  department_39  department_40  \
0              0              0              0              0              0   

   department_41  
0              0  

[1 rows x 50 columns]
#+end_example

** Splitting the Data

For evaluation purposes I'll use the traditional train-test split.

#+BEGIN_SRC ipython :session emailmodel :results output
x_data = cleaned_data.loc[:, cleaned_data.columns != "management"]

y_data = cleaned_data.management

print(x_data.head())
print(y_data.head())
#+END_SRC

#+RESULTS:
#+begin_example
   clustering  degree  degree_centrality  closeness_centrality  \
0    0.276423      44           0.043825              0.421991   
3    0.384910      71           0.070717              0.441663   
4    0.318691      96           0.095618              0.462152   
6    0.155183     115           0.114542              0.475805   
7    0.287785      72           0.071713              0.420156   

   betweenness_centrality  pagerank  authority  department_0  department_1  \
0                0.001124  0.001224   0.000944             0             1   
3                0.001654  0.001833   0.002369             0             0   
4                0.005547  0.002526   0.003055             0             0   
6                0.012387  0.003146   0.002554             0             0   
7                0.002818  0.002002   0.001155             0             0   

   department_2      ...        department_32  department_33  department_34  \
0             0      ...                    0              0              0   
3             0      ...                    0              0              0   
4             0      ...                    0              0              0   
6             0      ...                    0              0              0   
7             0      ...                    0              0              0   

   department_35  department_36  department_37  department_38  department_39  \
0              0              0              0              0              0   
3              0              0              0              0              0   
4              0              0              0              0              0   
6              0              0              0              0              0   
7              0              0              0              0              0   

   department_40  department_41  
0              0              0  
3              0              0  
4              0              0  
6              0              0  
7              0              0  

[5 rows x 49 columns]
0    0.0
3    1.0
4    1.0
6    1.0
7    0.0
Name: management, dtype: float64
#+end_example

#+BEGIN_SRC ipython :session emailmodel :results output
print(y_data.value_counts())
#+END_SRC

#+RESULTS:
: 0.0    634
: 1.0    119
: Name: management, dtype: int64

#+BEGIN_SRC ipython :session emailmodel :file /tmp/management_bar.png
seaborn.countplot(x='management', data=cleaned_data)
#+END_SRC

#+RESULTS:
[[file:/tmp/management_bar.png]]

It looks like the management data is unbalanced, so I'll do a stratified split.

#+BEGIN_SRC ipython :session emailmodel :results output
x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, stratify=y_data)
print(x_train.shape)
print(y_test.shape)
#+END_SRC

#+RESULTS:
: (564, 49)
: (189,)

#+BEGIN_SRC ipython :session emailmodel :file /tmp/management_train.png
seaborn.countplot(y_train)
#+END_SRC

#+RESULTS:
[[file:/tmp/management_train.png]]

Looks close enough for government work.

* Dummy Classifier
  As a baseline I'll use a [[http://scikit-learn.org/stable/modules/model_evaluation.html#dummy-estimators][Dummy Classifier]] which uses a simple rule rather than the input data to make predictions.

#+BEGIN_SRC ipython :session emailmodel :results none
parameter_grid = dict(strategy=["stratified", 'most_frequent', 'prior', 'uniform'])
#+END_SRC

Now we'll do a grid search.

#+BEGIN_SRC ipython :session emailmodel :results none
grid_search = GridSearchCV(DummyClassifier(), parameter_grid,
                           cv=StratifiedKFold(10), scoring="roc_auc")
grid_search.fit(x_train, y_train)
#+END_SRC

#+BEGIN_SRC ipython :session emailmodel :results output
BASELINE = grid_search.score(x_test, y_test)
print(grid_search.best_params_)
print(BASELINE)
#+END_SRC

#+RESULTS:
: {'strategy': 'stratified'}
: 0.551572327044

It looks like it chose the *stratified* strategy, which should predict that the instances are all non-managers. Our baseline AUC score is 0.5.

#+BEGIN_SRC ipython :session emailmodel :results output
results = pandas.DataFrame(grid_search.cv_results_)
print(results.head(1))
#+END_SRC

#+RESULTS:
#+begin_example
   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \
0       0.002192         0.001767         0.508104          0.515411   

  param_strategy                      params  rank_test_score  \
0     stratified  {'strategy': 'stratified'}                1   

   split0_test_score  split0_train_score  split1_test_score       ...         \
0           0.538194            0.486783           0.482639       ...          

   split7_test_score  split7_train_score  split8_test_score  \
0           0.414894            0.506484           0.515366   

   split8_train_score  split9_test_score  split9_train_score  std_fit_time  \
0            0.516297           0.529255            0.526999      0.000314   

   std_score_time  std_test_score  std_train_score  
0        0.000463        0.055611         0.019322  

[1 rows x 31 columns]
#+end_example

#+BEGIN_SRC ipython :session emailmodel :file /tmp/dummy_scores.png
figure = pyplot.figure()
axe = figure.gca()
strategies = parameter_grid["strategy"]
x = pyplot.xticks(list(range(len(strategies))), strategies)
axe.plot(range(len(strategies)), results.mean_test_score)
axe.set_title("Dummy Classifier Strategy Vs AUC")
axe.set_xlabel("strategy")
axe.set_ylabel("AUC Score")
#+END_SRC

#+RESULTS:
[[file:/tmp/dummy_scores.png]]

So it looks like all the strategies except *stratified* did the same - and even the stratified did basically the same if you round it off.

* Standardizing the Data

#+BEGIN_SRC ipython :session emailmodel :results output
print(x_train.describe())
#+END_SRC

#+RESULTS:
#+begin_example
       clustering      degree  degree_centrality  closeness_centrality  \
count  564.000000  564.000000         564.000000            564.000000   
mean     0.398086   33.609929           0.033476              0.379251   
std      0.250952   36.283965           0.036139              0.076887   
min      0.000000    1.000000           0.000996              0.000000   
25%      0.270266    7.000000           0.006972              0.345993   
50%      0.377350   23.500000           0.023406              0.387551   
75%      0.520208   46.000000           0.045817              0.426084   
max      1.000000  234.000000           0.233068              0.531259   

       betweenness_centrality    pagerank     authority  department_0  \
count              564.000000  564.000000  5.640000e+02    564.000000   
mean                 0.001503    0.001007  9.965503e-04      0.051418   
std                  0.003606    0.000894  1.220955e-03      0.221046   
min                  0.000000    0.000171  4.675065e-58      0.000000   
25%                  0.000005    0.000359  1.786180e-04      0.000000   
50%                  0.000183    0.000770  5.339403e-04      0.000000   
75%                  0.001329    0.001310  1.298304e-03      0.000000   
max                  0.037789    0.006069  7.491040e-03      1.000000   

       department_1  department_2      ...        department_32  \
count    564.000000    564.000000      ...           564.000000   
mean       0.062057      0.008865      ...             0.008865   
std        0.241473      0.093820      ...             0.093820   
min        0.000000      0.000000      ...             0.000000   
25%        0.000000      0.000000      ...             0.000000   
50%        0.000000      0.000000      ...             0.000000   
75%        0.000000      0.000000      ...             0.000000   
max        1.000000      1.000000      ...             1.000000   

       department_33  department_34  department_35  department_36  \
count          564.0     564.000000     564.000000     564.000000   
mean             0.0       0.014184       0.005319       0.012411   
std              0.0       0.118356       0.072803       0.110811   
min              0.0       0.000000       0.000000       0.000000   
25%              0.0       0.000000       0.000000       0.000000   
50%              0.0       0.000000       0.000000       0.000000   
75%              0.0       0.000000       0.000000       0.000000   
max              0.0       1.000000       1.000000       1.000000   

       department_37  department_38  department_39  department_40  \
count     564.000000     564.000000     564.000000     564.000000   
mean        0.021277       0.012411       0.003546       0.001773   
std         0.144433       0.110811       0.059496       0.042108   
min         0.000000       0.000000       0.000000       0.000000   
25%         0.000000       0.000000       0.000000       0.000000   
50%         0.000000       0.000000       0.000000       0.000000   
75%         0.000000       0.000000       0.000000       0.000000   
max         1.000000       1.000000       1.000000       1.000000   

       department_41  
count          564.0  
mean             0.0  
std              0.0  
min              0.0  
25%              0.0  
50%              0.0  
75%              0.0  
max              0.0  

[8 rows x 49 columns]
#+end_example

#+BEGIN_SRC ipython :session emailmodel :results none
scaler = StandardScaler()
scaler.fit(x_train)
x_train = pandas.DataFrame(scaler.transform(x_train), columns=x_train.columns)
x_test = scaler.transform(x_test)
#+END_SRC

#+BEGIN_SRC ipython :session emailmodel :results output
print(x_train.describe())
#+END_SRC

#+RESULTS:
#+begin_example
         clustering        degree  degree_centrality  closeness_centrality  \
count  5.640000e+02  5.640000e+02       5.640000e+02          5.640000e+02   
mean   5.826702e-17  5.590485e-17       8.740054e-17         -3.070830e-16   
std    1.000888e+00  1.000888e+00       1.000888e+00          1.000888e+00   
min   -1.587708e+00 -8.995400e-01      -8.995400e-01         -4.936969e+00   
25%   -5.097899e-01 -7.340309e-01      -7.340309e-01         -4.329383e-01   
50%   -8.269899e-02 -2.788809e-01      -2.788809e-01          1.080513e-01   
75%    4.870696e-01  3.417782e-01       3.417782e-01          6.096613e-01   
max    2.400651e+00  5.527730e+00       5.527730e+00          1.978785e+00   

       betweenness_centrality      pagerank     authority  department_0  \
count            5.640000e+02  5.640000e+02  5.640000e+02  5.640000e+02   
mean             4.094440e-17 -5.118049e-17  6.850312e-17 -2.204698e-17   
std              1.000888e+00  1.000888e+00  1.000888e+00  1.000888e+00   
min             -4.172665e-01 -9.352089e-01 -8.169300e-01 -2.328210e-01   
25%             -4.158186e-01 -7.247416e-01 -6.705065e-01 -2.328210e-01   
50%             -3.664423e-01 -2.647417e-01 -3.792282e-01 -2.328210e-01   
75%             -4.823649e-02  3.394482e-01  2.473652e-01 -2.328210e-01   
max              1.007206e+01  5.665736e+00  5.323910e+00  4.295146e+00   

       department_1  department_2      ...        department_32  \
count  5.640000e+02  5.640000e+02      ...         5.640000e+02   
mean   2.362177e-18 -1.102349e-17      ...        -3.149569e-18   
std    1.000888e+00  1.000888e+00      ...         1.000888e+00   
min   -2.572209e-01 -9.457560e-02      ...        -9.457560e-02   
25%   -2.572209e-01 -9.457560e-02      ...        -9.457560e-02   
50%   -2.572209e-01 -9.457560e-02      ...        -9.457560e-02   
75%   -2.572209e-01 -9.457560e-02      ...        -9.457560e-02   
max    3.887710e+00  1.057355e+01      ...         1.057355e+01   

       department_33  department_34  department_35  department_36  \
count          564.0   5.640000e+02   5.640000e+02   5.640000e+02   
mean             0.0  -4.015700e-17  -1.574784e-18  -6.299138e-18   
std              0.0   1.000888e+00   1.000888e+00   1.000888e+00   
min              0.0  -1.199520e-01  -7.312724e-02  -1.121041e-01   
25%              0.0  -1.199520e-01  -7.312724e-02  -1.121041e-01   
50%              0.0  -1.199520e-01  -7.312724e-02  -1.121041e-01   
75%              0.0  -1.199520e-01  -7.312724e-02  -1.121041e-01   
max              0.0   8.336666e+00   1.367479e+01   8.920282e+00   

       department_37  department_38  department_39  department_40  \
count   5.640000e+02     564.000000   5.640000e+02   5.640000e+02   
mean   -2.519655e-17       0.000000   3.779483e-17   6.299138e-18   
std     1.000888e+00       1.000888   1.000888e+00   1.000888e+00   
min    -1.474420e-01      -0.112104  -5.965500e-02  -4.214498e-02   
25%    -1.474420e-01      -0.112104  -5.965500e-02  -4.214498e-02   
50%    -1.474420e-01      -0.112104  -5.965500e-02  -4.214498e-02   
75%    -1.474420e-01      -0.112104  -5.965500e-02  -4.214498e-02   
max     6.782330e+00       8.920282   1.676305e+01   2.372762e+01   

       department_41  
count          564.0  
mean             0.0  
std              0.0  
min              0.0  
25%              0.0  
50%              0.0  
75%              0.0  
max              0.0  

[8 rows x 49 columns]
#+end_example

* Feature Selection
  I'm going to need to do some feature reduction, but figuring out what is important and what isn't is something I'm going to have to leave to the machine. I'm going to assume that the features thrown out by logistic regression with l1 penalization are unimportant. 

#+BEGIN_SRC ipython :session emailmodel :results none
logistic_model = LogisticRegressionCV(penalty='l1',
                                      solver='liblinear', scoring="roc_auc")
logistic_model.fit(x_train, y_train)
model = SelectFromModel(logistic_model, prefit=True)

x_train_positive = model.transform(x_train)
x_test_positive = model.transform(x_test)
#+END_SRC

#+BEGIN_SRC ipython :session emailmodel :results output
print(logistic_model.score(x_test, y_test))
#+END_SRC

#+RESULTS:
: 0.915343915344

Logistic Regression with =L1= penalty seems to do reasonably well even without feature selection.

#+BEGIN_SRC ipython :session emailmodel :results output
logistic_model.fit(x_train_positive, y_train)
print(logistic_model.score(x_test_positive, y_test))
#+END_SRC

#+RESULTS:
: 0.915343915344

It looks like feature selection didn't really help here.

#+BEGIN_SRC ipython :session emailmodel :results output
print(x_train.shape)
print(x_train_positive.shape)
print(model.ranking_)
#+END_SRC

#+RESULTS:
: (564, 49)
: (564, 39)

As a double-check I'll use a tree-based, recursive feature-elimination version.

#+BEGIN_SRC ipython :session emailmodel :results none
trees = ExtraTreesClassifier(n_estimators=10)
eliminator = RFECV(estimator=trees, cv=StratifiedKFold(10), scoring="roc_auc")
eliminator.fit(x_train, y_train)
x_train_trees = eliminator.transform(x_train)
x_test_trees = eliminator.transform(x_test)
#+END_SRC

#+BEGIN_SRC ipython :session emailmodel :results output
print(x_train_trees.shape)
print(eliminator.ranking_)
#+END_SRC

#+RESULTS:
: (564, 17)
: [ 1  1  1  1  1  1  1  7  5 28 27  1 19  1  3 17 15  1  2 21  1  4  1 13 12
:  32  1  9  1  1 25  1 24 29  6  1 26 10 30 11 31 18 16 14  8 20 22 23 33]

This eliminated many more columns than the Logistic Regression version did.

*warning* this seem to change every time you run it - the randomness changes it. Only the elimination of the first column seems to do as well as not running it at all.

* Fit and Display
  This is a convenience function so I can fit and display the scores for the models.

#+BEGIN_SRC ipython :session emailmodel :results none
def fit_and_display(model, identifier):
    """Fit and display the scores

    Args:
     model: The instantiated model to fit
     identifier (str): something to output at the beginning
    """
    print(identifier)
    print("=" * len(identifier))
    model.fit(x_train, y_train)
    print("\nX-train")
    print("Score: {:.2f}".format(model.score(x_test, y_test)))
    print("\nX-Train Positive")
    model.fit(x_train_positive, y_train)
    print("Score: {:.2f}".format(model.score(x_test_positive, y_test)))
    print("\nX-Train Trees")
    model.fit(x_train_trees, y_train)
    print("Score: {:.2f}".format(model.score(x_test_trees, y_test)))
    return
#+END_SRC  

* Logistic Regression
** L1 Penalty

#+BEGIN_SRC ipython :session emailmodel :results output
model = LogisticRegressionCV(penalty="l1", scoring="roc_auc", solver="liblinear")
fit_and_display(model, "Logistic Regression L1")
#+END_SRC

#+RESULTS:
#+begin_example
Logistic Regression L1
======================

X-train
Score: 0.92

X-Train Positive
Score: 0.92

X-Train Trees
Score: 0.89
#+end_example

  I've already run the Logistic Regression using a 'l1' but I'll try it again with 'l2' to see if it improved.

#+BEGIN_SRC ipython :session emailmodel :results output
model = LogisticRegressionCV(scoring="roc_auc", solver="liblinear")
fit_and_display(model, "LogisticRegression")
#+END_SRC

#+RESULTS:
#+begin_example
LogisticRegression
==================

X-train
Score: 0.89

X-Train Positive
Score: 0.89

X-Train Trees
Score: 0.91
#+end_example

L1 seems to do better than L1 overall, although it doesn't do as well with the recursively data form some reason.


* Random Forests
  I'll try a [[http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html][Random Forest]] classifier next.

#+BEGIN_SRC ipython :session emailmodel :results output
parameter_grid = dict(n_estimators=range(10, 100, 10))
search = GridSearchCV(RandomForestClassifier(), parameter_grid,
                      cv=StratifiedKFold(10), scoring="roc_auc")
fit_and_display(search, "Random Forest")
#+END_SRC

#+RESULTS:
#+begin_example
Random Forest
=============

X-train
Score: 0.95

X-Train Positive
Score: 0.95

X-Train Trees
Score: 0.96
#+end_example

This seems to have done much better than the logistic regression did. My logistic-regression feature reduction doesn't seem to help.

#+BEGIN_SRC ipython :session emailmodel :results none
class RandomForest(object):
    """trains a random forest on the x-test-trees set

    Args:
     start (int): first n-estimators value to use
     stop (int): last n-estimators value (minus step)
     step (int): amount to increment estimators
     folds (int): Cross-validation-folds to usen

    Returns:
     GridSearchCV: grid-search with the best estimator
    """

    def __init__(self, start, stop, step, folds=10):
        self.start = start
        self.stop = stop
        self.step = step
        self.folds = folds
        self._search = None
        self._parameter_grid = None
        return

    @property
    def parameter_grid(self):
        """dict of the number of estimators to use"""
        if self._parameter_grid is None:
            self._parameter_grid = dict(n_estimators=list(range(self.start,
                                                                self.stop,
                                                                self.step)))
        return self._parameter_grid

    @property
    def search(self):
        """grid-search cv object"""
        if self._search is None:
            self._search = GridSearchCV(RandomForestClassifier(),
                                        self.parameter_grid,
                                        cv=StratifiedKFold(self.folds),
                                        scoring="roc_auc")
        return self._search    

    def fit(self):
        """fits the model to the tree-based reduced-feature data"""
        self.search.fit(x_train_trees, y_train)
        print(self.search.score(x_test_trees, y_test))
        print(self.search.best_estimator_.feature_importances_)
        print(self.search.best_params_)
        return

    def plot(self):
        """Plots estimators vs AUC scores"""
        figure = pyplot.figure()
        axe = figure.gca()
        axe.plot(self.parameter_grid["n_estimators"],
                 self.search.cv_results_["mean_test_score"])
        axe.set_title("Estimator Count vs AUC")
        axe.set_xlabel("Number of estimators (trees)")
        axe.set_ylabel("Mean AUC Score")
        return
#+END_SRC

#+BEGIN_SRC ipython :session emailmodel :results output
search = RandomForest(10, 100, 10)
search.fit()
#+END_SRC

#+RESULTS:
: 0.9535639413
: [ 0.13812182  0.09755737  0.09813092  0.11238756  0.21084885  0.13873895
:   0.13490098  0.00642729  0.00618458  0.01627777  0.00537357  0.00819071
:   0.00314849  0.0059638   0.00448527  0.00746673  0.00579533]
: {'n_estimators': 90}

Not a lot of variance in the importance of the features.

#+BEGIN_SRC ipython :session emailmodel :file /tmp/random_forest.png
search.plot()
#+END_SRC

#+RESULTS:
[[file:/tmp/random_forest.png]]

Would things get better with more trees?

#+BEGIN_SRC ipython :session emailmodel :results output
search = RandomForest(150, 250, 10)
search.fit()
#+END_SRC

#+RESULTS:
: 0.957442348008
: [ 0.13953651  0.09979475  0.09186246  0.12867179  0.20912476  0.12450353
:   0.14282428  0.00641921  0.00454248  0.01450143  0.00592516  0.00713243
:   0.00253501  0.00851197  0.00478764  0.0058287   0.00349789]
: {'n_estimators': 150}

#+BEGIN_SRC ipython :session emailmodel :file /tmp/random_forest_2.png
search.plot()
#+END_SRC

#+RESULTS:
[[file:/tmp/random_forest_2.png]]

In this case the test-score was better, although the training scores don't look much better. I guess it's the randomness coming into play again. I'll try a long run instead.

#+BEGIN_SRC ipython :session emailmodel :results output
search = RandomForest(10, 500, 10)
search.fit()
#+END_SRC

#+RESULTS:
: 0.954192872117
: [ 0.13424954  0.08315042  0.09720588  0.13162792  0.20094993  0.13482168
:   0.14807443  0.00651954  0.00569103  0.01637417  0.00562885  0.00823928
:   0.00241943  0.00887233  0.0043307   0.00718     0.00466488]
: {'n_estimators': 190}

#+BEGIN_SRC ipython :session emailmodel :file /tmp/random_forest_long.png
search.plot()
#+END_SRC

#+RESULTS:
[[file:/tmp/random_forest_long.png]]

The test-score for the best estimator is actually a little worse than it was for the previous case, although it's qute a small difference.

* K Nearest Neighbors

#+BEGIN_SRC ipython :session emailmodel :results none
parameters = dict(n_neighbors=range(10, 20),
                  weights=["uniform", "distance"],
                  p=[1, 2],
                  leaf_size=range(10, 50, 10))

search = GridSearchCV(KNeighborsClassifier(), parameters, scoring="roc_auc")
search.fit(x_train_trees, y_train)
#+END_SRC

#+BEGIN_SRC ipython :session emailmodel :results output
print(search.score(x_test_trees, y_test))
print(search.best_params_)
#+END_SRC

#+RESULTS:
: 0.802096436059
: {'leaf_size': 10, 'n_neighbors': 17, 'p': 1, 'weights': 'distance'}

This doesn't seem to do so well, although I'm not as experienced at using it so I might be using bad parameters.

* Support Vector Classifier (SVC)

#+BEGIN_SRC ipython :session emailmodel :results output
parameters = dict(C=numpy.arange(.1, 1, 0.1), gamma=range(1, 10, 1),
                  kernel=["linear", 'rbf', 'sigmoid'])
search = GridSearchCV(SVC(class_weight='balanced'), parameters, scoring='roc_auc')
fit_and_display(search, "SVC")
#+END_SRC

#+RESULTS:
#+begin_example
SVC
===

X-train
Score: 0.90

X-Train Positive
Score: 0.91

X-Train Trees
Score: 0.91
#+end_example

#+BEGIN_SRC ipython :session emailmodel :results output
print(search.score(x_test_trees, y_test))
print(search.best_params_)
#+END_SRC

#+RESULTS:
: 0.909014675052
: {'C': 0.90000000000000002, 'gamma': 1, 'kernel': 'linear'}

Now that the data is scaled, the svc does much better, alhough still not as well as the random forest.

