#+TITLE: Random Graphs

* Imports

#+BEGIN_SRC ipython :session randomgraphs :results none
# python standard library
import os
import pickle

# from pypi
import networkx
import numpy
import pandas

from sklearn.linear_model import LogisticRegressionCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import (
    ExtraTreesClassifier,
    RandomForestClassifier,
    )
from sklearn.feature_selection import (
    RFECV,
)
from sklearn.model_selection import (
    GridSearchCV,
    StratifiedKFold,
    train_test_split,
    )
#+END_SRC

#+BEGIN_SRC ipython :session randomgraphs :results none
% matplotlib inline
#+END_SRC

* Part 1 - Random Graph Identification
 
For the first part of this assignment you will analyze randomly generated graphs and determine which algorithm created them.

** Load the data

#+BEGIN_SRC ipython :session randomgraphs :results output
part_one_graphs = pickle.load(open('A4_graphs','rb'))
print(len(part_one_graphs))
print(type(part_one_graphs[0]))
#+END_SRC

#+RESULTS:
: 5
: <class 'networkx.classes.graph.Graph'>

=part_one_graphs= is a list containing 5 networkx graphs. Each of these graphs were generated by one of three possible algorithms:

 - Preferential Attachment (`'PA'`)
 - Small World with low probability of rewiring (`'SW_L'`)
 - Small World with high probability of rewiring (`'SW_H'`)

Analyze each of the 5 graphs and determine which of the three algorithms generated the graph.

*The `graph_identification` function should return a list of length 5 where each element in the list is either `'PA'`, `'SW_L'`, or `'SW_H'`.*

** Graph Identification
#+BEGIN_SRC ipython :session randomgraphs :results none
def graph_identification():
    """Identifies the type of graph each of the graphs is

    Returns:
     list: string identifiers for the type of graph
    """
    graph_types = []
    for graph in part_one_graphs:
        path = networkx.average_shortest_path_length(graph)
        coefficient = networkx.average_clustering(graph)
        if path > 6:
            if coefficient < 0.5:
                graph_types.append("SW_L")
            else:
                raise Exception("unexpected type")
        else:
            if coefficient < 0.5:
                graph_types.append("PA")
            else:
                graph_types.append("SW_H")
    return graph_types
#+END_SRC

* Part 2 - Company Emails

For the second part of this assignment you will be working with a company's email network where each node corresponds to a person at the company, and each edge indicates that at least one email has been sent between two people.

The network also contains the node attributes `Department` and `ManagementSalary`.

`Department` indicates the department in the company which the person belongs to, and `ManagementSalary` indicates whether that person is receiving a managment position salary.

#+BEGIN_SRC ipython :session randomgraphs :results output
email = networkx.read_gpickle('email_prediction.txt')
print(networkx.info(email))
#+END_SRC

#+RESULTS:
: Name: 
: Type: Graph
: Number of nodes: 1005
: Number of edges: 16706
: Average degree:  33.2458

** Part 2A - Salary Prediction

Using network `email`, identify the people in the network with missing values for the node attribute `ManagementSalary` and predict whether or not these individuals are receiving a managment position salary.

To accomplish this, you will need to create a matrix of node features using networkx, train a sklearn classifier on nodes that have `ManagementSalary` data, and predict a probability of the node receiving a managment salary for nodes where `ManagementSalary` is missing.

Your predictions will need to be given as the probability that the corresponding employee is receiving a managment position salary.

The evaluation metric for this assignment is the Area Under the ROC Curve (AUC).

Your grade will be based on the AUC score computed for your classifier. A model which with an AUC of 0.75 or higher will receive full points.

Using your trained classifier, return a series of length 252 with the data being the probability of receiving managment salary, and the index being the node id.
 
#+BEGIN_EXAMPLE  
      1       1.0
      2       0.0
      5       0.8
      8       1.0
          ...
      996     0.7
      1000    0.5
      1001    0.0
      Length: 252, dtype: float64
#+END_EXAMPLE

*** The Data Frame
#+BEGIN_SRC ipython :session randomgraphs :results output
if not os.path.isfile("email_data.h5"):
    data = pandas.DataFrame(index=email.nodes())
    data["department"] = pandas.Series(networkx.get_node_attributes(email, "Department"))
    data["management"] = pandas.Series(networkx.get_node_attributes(email, "ManagementSalary"))
    data["clustering"] = pandas.Series(networkx.clustering(email))
    data["degree"] = pandas.Series(email.degree())
    data["degree_centrality"] = pandas.Series(networkx.degree_centrality(email))
    data["closeness_centrality"] = pandas.Series(networkx.closeness_centrality(email))
    data["betweenness_centrality"] = pandas.Series(networkx.betweenness_centrality(email))
    data["pagerank"] = pandas.Series(networkx.pagerank(email))
    _, authority = networkx.hits(email)
    data["authority"] = pandas.Series(authority)
    data.to_hdf("email_data.h5","df" )
else:
    data = pandas.read_hdf('email_data.h5', "df")
print(data.head())    
#+END_SRC

#+RESULTS:
#+begin_example
   department  management  clustering  degree  degree_centrality  \
0           1         0.0    0.276423      44           0.043825   
1           1         NaN    0.265306      52           0.051793   
2          21         NaN    0.297803      95           0.094622   
3          21         1.0    0.384910      71           0.070717   
4          21         1.0    0.318691      96           0.095618   

   closeness_centrality  betweenness_centrality  pagerank  authority  
0              0.421991                0.001124  0.001224   0.000944  
1              0.422360                0.001195  0.001426   0.001472  
2              0.461490                0.006570  0.002605   0.002680  
3              0.441663                0.001654  0.001833   0.002369  
4              0.462152                0.005547  0.002526   0.003055  
#+end_example

#+BEGIN_SRC ipython :session randomgraphs :results output
print(data.management.unique())
print(data.department.unique())
#+END_SRC

#+RESULTS:
: [  0.  nan   1.]
: [ 1 21 25 14  9 26  4 17 34 11  5 10 36 37  7 22  8 15  3 29 20 16 38 27 13
:   6  0 28  2 40 35 23 19 24 32 31 39 12 30 41 18 33]

*** Department Dummy Variables
   Even though I don't think it's going to prove useful, the =department= feature is actually categorical, despite the use of integers so we'll have to use One-Hot-Encoding to add dummy variables for it.

#+BEGIN_SRC ipython :session randomgraphs :results output
dummies_data = pandas.get_dummies(data, columns=["department"])
print(dummies_data.head(1))
#+END_SRC

#+RESULTS:
#+begin_example
   management  clustering  degree  degree_centrality  closeness_centrality  \
0         0.0    0.276423      44           0.043825              0.421991   

   betweenness_centrality  pagerank  authority  department_0  department_1  \
0                0.001124  0.001224   0.000944             0             1   

       ...        department_32  department_33  department_34  department_35  \
0      ...                    0              0              0              0   

   department_36  department_37  department_38  department_39  department_40  \
0              0              0              0              0              0   

   department_41  
0              0  

[1 rows x 50 columns]
#+end_example

*** Separating the Training and Prediction Sets
    We're going to use the model to predict what the missing =management= values are so I'm going to separate the missing and non-missing sets. 

#+BEGIN_SRC ipython :session randomgraphs :results output
training_data = dummies_data[pandas.notnull(dummies_data.management)]
prediction_data = dummies_data[pandas.isnull(dummies_data.management)]
print(training_data.shape)
print(prediction_data.shape)
#+END_SRC

#+RESULTS:
: (753, 50)
: (252, 50)

The problem description tells us that the answer should have 252 entries so this is a safe assertion.

#+BEGIN_SRC ipython :session randomgraphs :results none
assert len(prediction_data) == 252
#+END_SRC

*** Training and Target Data
    To train the model we'll need to separate out the =management= column (and remove it entirely from the =prediction= set).

#+BEGIN_SRC ipython :session randomgraphs :results none
non_management = [column for column in training_data.columns if column != "management"]
y_train = training_data.management
x_train = training_data[non_management]
x_predict = prediction_data[non_management]
#+END_SRC

*** Scaling
    I don't think the Random Forest model that I'm going to use needs it, but I'm going to standardize the data.

#+BEGIN_SRC ipython :session randomgraphs :results none
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_predict = pandas.DataFrame(scaler.transform(x_predict), index=x_predict.index)
#+END_SRC

*** Feature Selection
    Since we now have so many features, I'm going to do some feature selection.

#+BEGIN_SRC ipython :session randomgraphs :results output
print(x_train.shape)
print(x_predict.shape)
trees = ExtraTreesClassifier(n_estimators=10)
eliminator = RFECV(estimator=trees, cv=StratifiedKFold(10), scoring="roc_auc")
eliminator.fit(x_train, y_train)
x_train_reduced = eliminator.transform(x_train)
x_predict_reduced = pandas.DataFrame(eliminator.transform(x_predict), index=x_predict.index)
print(x_train_reduced.shape)
print(x_predict_reduced.shape)
#+END_SRC

#+RESULTS:
: (753, 49)
: (252, 49)
: (753, 9)
: (252, 9)

When I used the train-test-split training model it left 17 columns. I wonder if using the whole training set messes it up.

*** Logistic Regression

#+BEGIN_SRC ipython :session randomgraphs :results output
model = LogisticRegressionCV(penalty="l1", scoring="roc_auc",
                             solver="liblinear", cv=StratifiedKFold(10))
model.fit(x_train_reduced, y_train)
print(model.scores_[1.0].mean())
print(model.scores_[1.0].std())
#+END_SRC

#+RESULTS:
: 0.801941720028
: 0.2034340985

It seems to be doing much worse than when I used the train-test split.

*** Random Forests
#+BEGIN_SRC ipython :session randomgraphs :results output
parameter_grid = dict(n_estimators=range(10, 100, 10))
search = GridSearchCV(RandomForestClassifier(), parameter_grid,
                      cv=StratifiedKFold(10), scoring="roc_auc")
search.fit(x_train_reduced, y_train)
print(search.best_score_)
#+END_SRC

#+RESULTS:
: 0.971585130138

*** Submission

#+BEGIN_SRC ipython :session randomgraphs :results none
def salary_predictions():
    """Prediction that employee is management

    Calculates the probability that an employee is management
    
    Returns:
     pandas.Series: Node ID, probability of node
    """
    # probabilites is an array with rows of 
    # [<probability not management>, <probability management>]
    # see search.best_estimator_.classes_ to see what each entry represents
    probabilities = search.best_estimator_.predict_proba(x_predict_reduced)
    return pandas.Series(probabilities[:, 1], index=x_predict_reduced.index)
#+END_SRC

#+BEGIN_SRC ipython :session randomgraphs :results output
output = salary_predictions()
print(output.head())
#+END_SRC

#+RESULTS:
: 1     0.016667
: 2     0.950000
: 5     1.000000
: 8     0.150000
: 14    0.066667
: dtype: float64

** Part 2B - New Connections Prediction

# For the last part of this assignment, you will predict future connections between employees of the network. The future connections information has been loaded into the variable `future_connections`. The index is a tuple indicating a pair of nodes that currently do not have a connection, and the `Future Connection` column indicates if an edge between those two nodes will exist in the future, where a value of 1.0 indicates a future connection.

# In[ ]:


future_connections = pd.read_csv('Future_Connections.csv', index_col=0, converters={0: eval})
future_connections.head(10)


# Using network `G` and `future_connections`, identify the edges in `future_connections` with missing values and predict whether or not these edges will have a future connection.
# 
# To accomplish this, you will need to create a matrix of features for the edges found in `future_connections` using networkx, train a sklearn classifier on those edges in `future_connections` that have `Future Connection` data, and predict a probability of the edge being a future connection for those edges in `future_connections` where `Future Connection` is missing.
# 
# 
# 
# Your predictions will need to be given as the probability of the corresponding edge being a future connection.
# 
# The evaluation metric for this assignment is the Area Under the ROC Curve (AUC).
# 
# Your grade will be based on the AUC score computed for your classifier. A model which with an AUC of 0.75 or higher will receive full points.
# 
# Using your trained classifier, return a series of length 122112 with the data being the probability of the edge being a future connection, and the index being the edge as represented by a tuple of nodes.
# 
#     Example:
#     
#         (107, 348)    0.35
#         (542, 751)    0.40
#         (20, 426)     0.55
#         (50, 989)     0.35
#                   ...
#         (939, 940)    0.15
#         (555, 905)    0.35
#         (75, 101)     0.65
#         Length: 122112, dtype: float64

# In[ ]:


def new_connections_predictions():
    
    # Your Code Here
    
    return # Your Answer Here



