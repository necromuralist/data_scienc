#+TITLE: Random Graphs

* Imports

#+BEGIN_SRC ipython :session randomgraphs :results none
# python standard library
import os
import pickle

# from pypi
import networkx
import numpy
import pandas

from sklearn.linear_model import LogisticRegressionCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import (
    ExtraTreesClassifier,
    RandomForestClassifier,
    )
from sklearn.feature_selection import (
    RFECV,
    SelectFromModel,
)
from sklearn.model_selection import (
    GridSearchCV,
    StratifiedKFold,
    train_test_split,
    )
#+END_SRC

#+BEGIN_SRC ipython :session randomgraphs :results none
% matplotlib inline
#+END_SRC

* Part 1 - Random Graph Identification
 
For the first part of this assignment you will analyze randomly generated graphs and determine which algorithm created them.

** Load the data

#+BEGIN_SRC ipython :session randomgraphs :results output
part_one_graphs = pickle.load(open('A4_graphs','rb'))
print(len(part_one_graphs))
print(type(part_one_graphs[0]))
#+END_SRC

#+RESULTS:
: 5
: <class 'networkx.classes.graph.Graph'>

=part_one_graphs= is a list containing 5 networkx graphs. Each of these graphs were generated by one of three possible algorithms:

 - Preferential Attachment (`'PA'`)
 - Small World with low probability of rewiring (`'SW_L'`)
 - Small World with high probability of rewiring (`'SW_H'`)

Analyze each of the 5 graphs and determine which of the three algorithms generated the graph.

*The `graph_identification` function should return a list of length 5 where each element in the list is either `'PA'`, `'SW_L'`, or `'SW_H'`.*

** Graph Identification
#+BEGIN_SRC ipython :session randomgraphs :results none
def graph_identification():
    """Identifies the type of graph each of the graphs is

    Returns:
     list: string identifiers for the type of graph
    """
    graph_types = []
    for graph in part_one_graphs:
        path = networkx.average_shortest_path_length(graph)
        coefficient = networkx.average_clustering(graph)
        if path > 6:
            if coefficient < 0.5:
                graph_types.append("SW_L")
            else:
                raise Exception("unexpected type")
        else:
            if coefficient < 0.5:
                graph_types.append("PA")
            else:
                graph_types.append("SW_H")
    return graph_types
#+END_SRC

This was marked wrong by the grader.

* Part 2 - Company Emails

For the second part of this assignment you will be working with a company's email network where each node corresponds to a person at the company, and each edge indicates that at least one email has been sent between two people.

The network also contains the node attributes `Department` and `ManagementSalary`.

`Department` indicates the department in the company which the person belongs to, and `ManagementSalary` indicates whether that person is receiving a managment position salary.

#+BEGIN_SRC ipython :session randomgraphs :results output
email = networkx.read_gpickle('email_prediction.txt')
print(networkx.info(email))
#+END_SRC

#+RESULTS:
: Name: 
: Type: Graph
: Number of nodes: 1005
: Number of edges: 16706
: Average degree:  33.2458

** Part 2A - Salary Prediction

Using network `email`, identify the people in the network with missing values for the node attribute `ManagementSalary` and predict whether or not these individuals are receiving a managment position salary.

To accomplish this, you will need to create a matrix of node features using networkx, train a sklearn classifier on nodes that have `ManagementSalary` data, and predict a probability of the node receiving a managment salary for nodes where `ManagementSalary` is missing.

Your predictions will need to be given as the probability that the corresponding employee is receiving a managment position salary.

The evaluation metric for this assignment is the Area Under the ROC Curve (AUC).

Your grade will be based on the AUC score computed for your classifier. A model which with an AUC of 0.75 or higher will receive full points.

Using your trained classifier, return a series of length 252 with the data being the probability of receiving managment salary, and the index being the node id.
 
#+BEGIN_EXAMPLE  
      1       1.0
      2       0.0
      5       0.8
      8       1.0
          ...
      996     0.7
      1000    0.5
      1001    0.0
      Length: 252, dtype: float64
#+END_EXAMPLE

*** The Data Frame
#+BEGIN_SRC ipython :session randomgraphs :results output
if not os.path.isfile("email_data.h5"):
    data = pandas.DataFrame(index=email.nodes())
    data["department"] = pandas.Series(networkx.get_node_attributes(email, "Department"))
    data["management"] = pandas.Series(networkx.get_node_attributes(email, "ManagementSalary"))
    data["clustering"] = pandas.Series(networkx.clustering(email))
    data["degree"] = pandas.Series(email.degree())
    data["degree_centrality"] = pandas.Series(networkx.degree_centrality(email))
    data["closeness_centrality"] = pandas.Series(networkx.closeness_centrality(email))
    data["betweenness_centrality"] = pandas.Series(networkx.betweenness_centrality(email))
    data["pagerank"] = pandas.Series(networkx.pagerank(email))
    _, authority = networkx.hits(email)
    data["authority"] = pandas.Series(authority)
    data.to_hdf("email_data.h5","df" )
else:
    data = pandas.read_hdf('email_data.h5', "df")
print(data.head())    
#+END_SRC

#+RESULTS:
#+begin_example
   department  management  clustering  degree  degree_centrality  \
0           1         0.0    0.276423      44           0.043825   
1           1         NaN    0.265306      52           0.051793   
2          21         NaN    0.297803      95           0.094622   
3          21         1.0    0.384910      71           0.070717   
4          21         1.0    0.318691      96           0.095618   

   closeness_centrality  betweenness_centrality  pagerank  authority  
0              0.421991                0.001124  0.001224   0.000944  
1              0.422360                0.001195  0.001426   0.001472  
2              0.461490                0.006570  0.002605   0.002680  
3              0.441663                0.001654  0.001833   0.002369  
4              0.462152                0.005547  0.002526   0.003055  
#+end_example

#+BEGIN_SRC ipython :session randomgraphs :results output
print(data.management.unique())
print(data.department.unique())
#+END_SRC

#+RESULTS:
: [  0.  nan   1.]
: [ 1 21 25 14  9 26  4 17 34 11  5 10 36 37  7 22  8 15  3 29 20 16 38 27 13
:   6  0 28  2 40 35 23 19 24 32 31 39 12 30 41 18 33]

*** Department Dummy Variables
   Even though I don't think it's going to prove useful, the =department= feature is actually categorical, despite the use of integers so we'll have to use One-Hot-Encoding to add dummy variables for it.

#+BEGIN_SRC ipython :session randomgraphs :results output
dummies_data = pandas.get_dummies(data, columns=["department"])
print(dummies_data.head(1))
#+END_SRC

#+RESULTS:
#+begin_example
   management  clustering  degree  degree_centrality  closeness_centrality  \
0         0.0    0.276423      44           0.043825              0.421991   

   betweenness_centrality  pagerank  authority  department_0  department_1  \
0                0.001124  0.001224   0.000944             0             1   

       ...        department_32  department_33  department_34  department_35  \
0      ...                    0              0              0              0   

   department_36  department_37  department_38  department_39  department_40  \
0              0              0              0              0              0   

   department_41  
0              0  

[1 rows x 50 columns]
#+end_example

*** Separating the Training and Prediction Sets
    We're going to use the model to predict what the missing =management= values are so I'm going to separate the missing and non-missing sets. 

#+BEGIN_SRC ipython :session randomgraphs :results output
training_data = dummies_data[pandas.notnull(dummies_data.management)]
prediction_data = dummies_data[pandas.isnull(dummies_data.management)]
print(training_data.shape)
print(prediction_data.shape)
#+END_SRC

#+RESULTS:
: (753, 50)
: (252, 50)

The problem description tells us that the answer should have 252 entries so this is a safe assertion.

#+BEGIN_SRC ipython :session randomgraphs :results none
assert len(prediction_data) == 252
#+END_SRC

*** Training and Target Data
    To train the model we'll need to separate out the =management= column (and remove it entirely from the =prediction= set).

#+BEGIN_SRC ipython :session randomgraphs :results none
non_management = [column for column in training_data.columns if column != "management"]
y_train = training_data.management
x_train = training_data[non_management]
x_predict = prediction_data[non_management]
#+END_SRC

*** Scaling
    I don't think the Random Forest model that I'm going to use needs it, but I'm going to standardize the data.

#+BEGIN_SRC ipython :session randomgraphs :results none
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_predict = pandas.DataFrame(scaler.transform(x_predict), index=x_predict.index)
#+END_SRC

*** Feature Selection
    Since we now have so many features, I'm going to do some feature selection.

#+BEGIN_SRC ipython :session randomgraphs :results output
print(x_train.shape)
print(x_predict.shape)
trees = ExtraTreesClassifier(n_estimators=10)
eliminator = RFECV(estimator=trees, cv=StratifiedKFold(10), scoring="roc_auc")
eliminator.fit(x_train, y_train)
x_train_reduced = eliminator.transform(x_train)
x_predict_reduced = pandas.DataFrame(eliminator.transform(x_predict), index=x_predict.index)
print(x_train_reduced.shape)
print(x_predict_reduced.shape)
#+END_SRC

#+RESULTS:
: (753, 49)
: (252, 49)
: (753, 9)
: (252, 9)

When I used the train-test-split training model it left 17 columns. I wonder if using the whole training set messes it up.

*** Logistic Regression

#+BEGIN_SRC ipython :session randomgraphs :results output
model = LogisticRegressionCV(penalty="l1", scoring="roc_auc",
                             solver="liblinear", cv=StratifiedKFold(10))
model.fit(x_train_reduced, y_train)
print(model.scores_[1.0].mean())
print(model.scores_[1.0].std())
#+END_SRC

#+RESULTS:
: 0.801941720028
: 0.2034340985

It seems to be doing much worse than when I used the train-test split.

*** Random Forests
#+BEGIN_SRC ipython :session randomgraphs :results output
parameter_grid = dict(n_estimators=range(10, 100, 10))
search = GridSearchCV(RandomForestClassifier(), parameter_grid,
                      cv=StratifiedKFold(10), scoring="roc_auc")
search.fit(x_train_reduced, y_train)
print(search.best_score_)
#+END_SRC

#+RESULTS:
: 0.971585130138

#+BEGIN_SRC ipython :session randomgraphs :results none
class RandomForest(object):
    """builds the random forest

    Args:
     x_train(array): data to train on
     y_train(array): targets for training
     start (int): start value for number of estimators
     stop (int): upper value for range of estimators
     step (int): increment for range of estimators
     folds (int): K-folds for cross-validation    
    """
    def __init__(self, x_train, y_train,
                 start=10, stop=100, step=10, folds=10):
        self.x_train = x_train
        self.y_train = y_train
        self.start = start
        self.stop = stop
        self.step = step
        self.folds = folds
        self._parameters = None
        self._search = None
        self._model = None
        return

    @property
    def parameters(self):
        """parameters for the grid-search"""
        if self._parameters is None:
            self._parameters = dict(n_estimators=range(self.start,
                                                       self.stop,
                                                       self.step))
        return self._parameters

    @property
    def search(self):
        """fitted grid search to find hyper-parameters"""
        if self._search is None:
            self._search = GridSearchCV(RandomForestClassifier(),
                                        self.parameters,
                                        cv=StratifiedKFold(self.folds),
                                        scoring="roc_auc")
            self._search.fit(self.x_train, self.y_train)
        return self._search

    @property
    def model(self):
        """best model found by the grid search"""
        if self._model is None:
            self._model = self.search.best_estimator_
        return self._model
#+END_SRC

*** Data Loader
    Since having all these org-babel things around makes things kind of hard I'm going to make a class to bundle everything together.

#+BEGIN_SRC ipython :session randomgraphs :results none
class DataLoader(object):
    """loads and transforms the data
    Args:
     estimators (int): number of trees to use for feature elimination
    """
    def __init__(self, estimators=10):
        self.estimators = estimators
        self._data = None
        self._dummies_data = None
        self._training_data = None
        self._prediction_data = None
        self._non_management = None
        self._y_train = None
        self._x_train = None
        self._x_predict = None
        self._scaler = None
        self._x_train_scaled = None
        self._x_predict_scaled = None
        self._eliminator = None
        self._x_train_reduced = None
        self._x_predict_reduced = None
        return

    @property
    def data(self):
        """The initial data"""
        if self._data is None:
            if not os.path.isfile("email_data.h5"):
                data = pandas.DataFrame(index=email.nodes())
                data["department"] = pandas.Series(networkx.get_node_attributes(email, "Department"))
                data["management"] = pandas.Series(networkx.get_node_attributes(email, "ManagementSalary"))
                data["clustering"] = pandas.Series(networkx.clustering(email))
                data["degree"] = pandas.Series(email.degree())
                data["degree_centrality"] = pandas.Series(networkx.degree_centrality(email))
                data["closeness_centrality"] = pandas.Series(networkx.closeness_centrality(email))
                data["betweenness_centrality"] = pandas.Series(networkx.betweenness_centrality(email))
                data["pagerank"] = pandas.Series(networkx.pagerank(email))
                _, authority = networkx.hits(email)
                data["authority"] = pandas.Series(authority)
                data.to_hdf("email_data.h5","df" )
                self._data = data
            else:
                self._data = pandas.read_hdf('email_data.h5', "df")
        return self._data

    @property
    def dummies_data(self):
        """one-hot-encoded data"""
        if self._dummies_data is None:
            self._dummies_data = pandas.get_dummies(self.data, columns=["department"])
        return self._dummies_data

    @property
    def training_data(self):
        """data with management information"""
        if self._training_data is None:
            self._training_data = self.dummies_data[pandas.notnull(
                self.dummies_data.management)]
        return self._training_data

    @property
    def prediction_data(self):
        """data missing management information"""
        if self._prediction_data is None:
            self._prediction_data = self.dummies_data[pandas.isnull(
                self.dummies_data.management)]
            assert len(self._prediction_data) == 252
        return self._prediction_data

    @property
    def non_management(self):
        """list of columns minus management"""
        if self._non_management is None:
            self._non_management = [
                column for column in self.training_data.columns
                if column != "management"]
        return self._non_management

    @property
    def y_train(self):
        """target-data for training"""
        if self._y_train is None:
            self._y_train = self.training_data.management
        return self._y_train

    @property
    def x_train(self):
        """data for training"""
        if self._x_train is None:
            self._x_train = self.training_data[self.non_management]
        return self._x_train

    @property
    def x_predict(self):
        """set to make predictions"""
        if self._x_predict is None:
            self._x_predict = self.prediction_data[self.non_management]
        return self._x_predict

    @property
    def scaler(self):
        """standard scaler"""
        if self._scaler is None:
            self._scaler = StandardScaler()
        return self._scaler

    @property
    def x_train_scaled(self):
        """training data scaled to 1 std, 0 mean"""
        if self._x_train_scaled is None:
            self._x_train_scaled = self.scaler.fit_transform(self.x_train)
        return self._x_train_scaled

    @property
    def x_predict_scaled(self):
        """prediction data with mean 0, std 1

        The answer requires the index so this is a dataframe
        instead of an array

        Returns:
         pandas.DataFrame: scaled data with index preserved
        """
        if self._x_predict_scaled is None:
            self._x_predict_scaled = pandas.DataFrame(
                self.scaler.transform(self.x_predict),
                index=self.x_predict.index)
        return self._x_predict_scaled

    @property
    def eliminator(self):
        """recursive feature eliminator"""
        if self._eliminator is None:
            trees = ExtraTreesClassifier(n_estimators=10)
            self._eliminator = RFECV(estimator=trees, cv=StratifiedKFold(10), 
                                     scoring="roc_auc")
            self._eliminator.fit(self.x_train_scaled, self.y_train)
        return self._eliminator

    @property
    def x_train_reduced(self):
        """training data with features eliminated"""
        if self._x_train_reduced is None:
            self._x_train_reduced = self.eliminator.transform(
                self.x_train_scaled)
        return self._x_train_reduced

    @property
    def x_predict_reduced(self):
        """prediction data with features eliminated"""
        if self._x_predict_reduced is None:
            self._x_predict_reduced = pandas.DataFrame(
                self.eliminator.transform(self.x_predict_scaled),
                index=self.x_predict_scaled.index)
        return self._x_predict_reduced
#+END_SRC

*** Submission
#+BEGIN_SRC ipython :session randomgraphs :results none
def salary_predictions():
    """Prediction that employee is management

    Calculates the probability that an employee is management
    
    Returns:
     pandas.Series: Node ID, probability of node
    """
    data = DataLoader()
    forest = RandomForest(data.x_train_reduced, data.y_train)
    # probabilites is an array with rows of 
    # [<probability not management>, <probability management>]
    # see forest.model.classes_ to see what each entry represents
    probabilities = forest.model.predict_proba(data.x_predict_reduced)
    return pandas.Series(probabilities[:, 1], index=data.x_predict_reduced.index)
#+END_SRC

#+BEGIN_SRC ipython :session randomgraphs :results output
output = salary_predictions()
print(output.head())
#+END_SRC

#+RESULTS:
: 1     0.033333
: 2     0.944444
: 5     1.000000
: 8     0.155556
: 14    0.022222
: dtype: float64

#+BEGIN_SRC ipython :session randomgraphs :results none
assert all(output.index == DataLoader().prediction_data.index)
assert len(output) == 252
#+END_SRC

** Part 2B - New Connections Prediction

For the last part of this assignment, you will predict future connections between employees of the network. The future connections information has been loaded into the variable `future_connections`. The index is a tuple indicating a pair of nodes that currently do not have a connection, and the `Future Connection` column indicates if an edge between those two nodes will exist in the future, where a value of 1.0 indicates a future connection.

#+BEGIN_SRC ipython :session randomgraphs :results none
future_connections = pandas.read_csv('Future_Connections.csv', index_col=0, converters={0: eval})
#+END_SRC

#+BEGIN_SRC ipython :session randomgraphs :results output
print(future_connections.head(10))
#+END_SRC

#+RESULTS:
#+begin_example
            Future Connection
(6, 840)                  0.0
(4, 197)                  0.0
(620, 979)                0.0
(519, 872)                0.0
(382, 423)                0.0
(97, 226)                 1.0
(349, 905)                0.0
(429, 860)                0.0
(309, 989)                0.0
(468, 880)                0.0
#+end_example

#+BEGIN_SRC ipython :session randomgraphs :results output
print(future_connections['Future Connection'].value_counts())
#+END_SRC

#+RESULTS:
: 0.0    337002
: 1.0     29332
: Name: Future Connection, dtype: int64


Using network `G` and `future_connections`, identify the edges in `future_connections` with missing values and predict whether or not these edges will have a future connection.

 To accomplish this, you will need to create a matrix of features for the edges found in `future_connections` using networkx, train a sklearn classifier on those edges in `future_connections` that have `Future Connection` data, and predict a probability of the edge being a future connection for those edges in `future_connections` where `Future Connection` is missing.

Your predictions will need to be given as the probability of the corresponding edge being a future connection.

The evaluation metric for this assignment is the Area Under the ROC Curve (AUC).

Your grade will be based on the AUC score computed for your classifier. A model which with an AUC of 0.75 or higher will receive full points.

Using your trained classifier, return a series of length 122112 with the data being the probability of the edge being a future connection, and the index being the edge as represented by a tuple of nodes.

#+BEGIN_EXAMPLE
         (107, 348)    0.35
         (542, 751)    0.40
         (20, 426)     0.55
         (50, 989)     0.35
                   ...
         (939, 940)    0.15
         (555, 905)    0.35
         (75, 101)     0.65
         Length: 122112, dtype: float64
#+END_EXAMPLE
*** Add Network Features

#+BEGIN_SRC ipython :session randomgraphs :results none
class Futures(object):
    target = "Future Connection"
    data_file = "Future_Connections.csv"
    graph_file = "email_prediction.txt"
    networkx_data_index = 2
    folds = 10
#+END_SRC

#+BEGIN_SRC ipython :session randomgraphs :results none
class DataNames(object):
    resource_allocation = 'resource_allocation'
    jaccard = 'jaccard_coefficient'
    adamic = "adamic_adar"
    preferential = "preferential_attachment"
#+END_SRC


#+BEGIN_SRC ipython :session randomgraphs :results none
def add_networkx_data(adder, name, graph=email, frame=future_connections):
    """Adds networkx data to the frame

    The networkx link-prediction functions return generators of triples:
     (first-node, second-node, value)

    This will use the index of the frame that's passed in as the source of 
    node-pairs for the networkx function (called `ebunch` in the networkx
    documentation) and the add only the value we want back to the frame

    Args:
     adder: networkx function to call to get the new data
     name: column-name to add to the frame
     graph: networkx graph to pass to the function
     frame (pandas.DataFrame): frame with node-pairs as index to add data to
    """
    frame[name] = [output[Futures.networkx_data_index]
                   for output in adder(graph, frame.index)]
    return frame
#+END_SRC

**** Adding A Resource Allocation Index

#+BEGIN_SRC ipython :session randomgraphs :results none
add_networkx_data(networkx.resource_allocation_index,
                  DataNames.resource_allocation)
#+END_SRC

#+BEGIN_SRC ipython :session randomgraphs :results output
print(future_connections.head(1))
#+END_SRC

#+RESULTS:
:           Future Connection  resource_allocation
: (6, 840)                0.0             0.136721

**** Adding the Jaccard Coefficient
#+BEGIN_SRC ipython :session randomgraphs :results none
add_networkx_data(networkx.jaccard_coefficient, DataNames.jaccard)
#+END_SRC

#+BEGIN_SRC ipython :session futures :results output
print(future_connections.head(1))
#+END_SRC

#+RESULTS:
:           Future Connection  resource_allocation  jaccard_coefficient  \
: (6, 840)                0.0             0.136721              0.07377   
: 
:           adamic_adar  preferential_attachment  
: (6, 840)     2.110314                     2070  

**** Adamic Adar

#+BEGIN_SRC ipython :session randomgraphs :results none
add_networkx_data(networkx.adamic_adar_index, DataNames.adamic)
#+END_SRC

#+BEGIN_SRC ipython :session futures :results output
print(future_connections.head(1))
#+END_SRC

#+RESULTS:
:           Future Connection  resource_allocation  jaccard_coefficient  \
: (6, 840)                0.0             0.136721              0.07377   
: 
:           adamic_adar  preferential_attachment  
: (6, 840)     2.110314                     2070  

**** Preferential Attachment
#+BEGIN_SRC ipython :session randomgraphs :results none
add_networkx_data(networkx.preferential_attachment, DataNames.preferential)
#+END_SRC

#+BEGIN_SRC ipython :session futures :results output
print(future_connections.head(1))
#+END_SRC

#+RESULTS:
:           Future Connection  resource_allocation  jaccard_coefficient  \
: (6, 840)                0.0             0.136721              0.07377   
: 
:           adamic_adar  preferential_attachment  
: (6, 840)     2.110314                     2070  

*** Setup the Training and Testing Data
**** Separating the Edges Without 'Future Connection' Values
   We are going to train on the values in the data with predictions and then make predictions for those that don't.

#+BEGIN_SRC ipython :session randomgraphs :results none
prediction_set = future_connections[future_connections[Futures.target].isnull()]
training_set = future_connections[future_connections[Futures.target].notnull()]
#+END_SRC

#+BEGIN_SRC ipython :session randomgraphs :results output
print(prediction_set.shape)
print(training_set.shape)
assert len(prediction_set) + len(training_set) == len(future_connections)
#+END_SRC

#+RESULTS:
: (122112, 5)
: (366334, 5)

** Separate the Target and Training Sets
#+BEGIN_SRC ipython :session randomgraphs :results none
non_target = [column for column in future_connections.columns
              if column != Futures.target]
x_train = training_set[non_target]
y_train = training_set[Futures.target]
x_predict = prediction_set[non_target]
#+END_SRC

#+BEGIN_SRC ipython :session randomgraphs :results none
assert all(x_train.columns == x_predict.columns)
assert len(x_train) == len(x_test)
#+END_SRC

** Scaling the Data
   To enable the use of linear models I'm going to scale the data so the mean is 0 and the variance is 1.

#+BEGIN_SRC ipython :session randomgraphs :results none
scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_predict_scaled = scaler.transform(x_predict)

x_train_frame = pandas.DataFrame(x_train_scaled, columns=x_train.columns)
x_predict_frame = pandas.DataFrame(x_predict_scaled, columns=x_predict.columns)
#+END_SRC

#+BEGIN_SRC ipython :session randomgraphs :results output
print(training.describe())
print(predictions.describe())
#+END_SRC

#+RESULTS:
#+begin_example
       resource_allocation  jaccard_coefficient   adamic_adar  \
count         3.663340e+05         3.663340e+05  3.663340e+05   
mean          2.362437e-17        -3.142158e-17  8.495464e-17   
std           1.000001e+00         1.000001e+00  1.000001e+00   
min          -3.787678e-01        -5.337500e-01 -4.313258e-01   
25%          -3.787678e-01        -5.337500e-01 -4.313258e-01   
50%          -3.787678e-01        -5.337500e-01 -4.313258e-01   
75%          -7.683777e-02         1.913300e-01  4.738833e-03   
max           6.213447e+01         2.629421e+01  4.468491e+01   

       preferential_attachment  
count             3.663340e+05  
mean              1.474099e-18  
std               1.000001e+00  
min              -5.442567e-01  
25%              -5.047032e-01  
50%              -3.711384e-01  
75%               7.713473e-02  
max               4.281843e+01  
       resource_allocation  jaccard_coefficient    adamic_adar  \
count        122112.000000        122112.000000  122112.000000   
mean              0.003738             0.004927       0.002951   
std               0.999686             1.013047       0.996963   
min              -0.378768            -0.533750      -0.431326   
25%              -0.378768            -0.533750      -0.431326   
50%              -0.378768            -0.533750      -0.431326   
75%              -0.070988             0.191330       0.007442   
max              43.017859            26.294210      32.196724   

       preferential_attachment  
count            122112.000000  
mean                  0.002366  
std                   1.008316  
min                  -0.544257  
25%                  -0.504703  
50%                  -0.372858  
75%                   0.074269  
max                  35.259698  
#+end_example
** Feature Selection
   To reduce the dimensionality I'm going to use model-based selection with Extra Trees.

#+BEGIN_SRC ipython :session randomgraphs :results output
estimator = ExtraTreesClassifier()
estimator.fit(x_train_scaled, y_train)
selector = SelectFromModel(estimator, prefit=True)
x_train_trees_sfm = selector.transform(x_train_scaled)
x_predict_sfm = selector.transform(x_predict_scaled)
print(estimator.feature_importances_)
#+END_SRC

#+RESULTS:
: [ 0.18060856  0.21380636  0.42918547  0.17639961]

#+BEGIN_SRC ipython :session randomgraphs :results output
print(x_train_trees_sfm.shape)
#+END_SRC

#+RESULTS:
: (366334, 1)

*** Missing Future Connections
#+BEGIN_SRC ipython :session randomgraphs :results none
model = LogisticRegressionCV(n_jobs=-1, scoring='roc_auc', solver='liblinear',
                             cv=StratifiedKFold())
model.fit(x_train_trees_sfm, y_train)
#+END_SRC

#+BEGIN_SRC ipython :session randomgraphs :results output
for scores in model.scores_[1.0]:
    print(max(scores))
    
#+END_SRC

#+RESULTS:
: 0.90631555176
: 0.90631556264
: 0.901334007951
: 0.901334019196
: 0.910627481378
: 0.910627490392

#+BEGIN_SRC ipython :session randomgraphs :results output
print(model.classes_)
#+END_SRC

#+RESULTS:
: [ 0.  1.]

#+BEGIN_SRC ipython :session randomgraphs :results none
def new_connections_predictions():    
    probabilities = model.predict_proba(x_predict_sfm)
    return pandas.Series(probabilities[:, 1], index=prediction_set.index)
#+END_SRC

#+BEGIN_SRC ipython :session randomgraphs :results output
outcome = new_connections_predictions()
assert len(outcome) == 122112, len(outcome)
print(outcome.head())
#+END_SRC

#+RESULTS:
: (107, 348)    0.056738
: (542, 751)    0.024142
: (20, 426)     0.552866
: (50, 989)     0.024142
: (942, 986)    0.024142
: dtype: float64


